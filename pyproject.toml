[project]
name = "dlmbench"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "accelerate==1.11.0",
    "datasets==3.6.0",
    "deepspeed>=0.18.2",
    "flash-attn",
    "hf-xet>=1.2.0",
    "lm-eval==0.4.8",
    "omegaconf>=2.3.0",
    "torch==2.8",
    "transformers==4.52.0",
]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp313-cp313-linux_x86_64.whl" }
